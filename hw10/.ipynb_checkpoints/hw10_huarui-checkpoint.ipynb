{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10, Part 1: Data Acquisition, Processing, and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Learning Objectives \n",
    "This presents how to build a neural network from scratch to classify images of rock-paper-scissors. During this lab we will explore the following concepts \n",
    "- how images are represented and handled in software\n",
    "- how to prepare a machine learning dataset\n",
    "- how a full machine learning pipeline looks\n",
    "- data preprocessing\n",
    "- data augmentation and its importance\n",
    "\n",
    "## Preparation \n",
    "Before we start the lab we'll need to install some python libaries. Open a terminal and run the installation commands found in the sites below. \n",
    "\n",
    "- **skimage**: https://scikit-image.org/download.html\n",
    "\n",
    "\n",
    "## Defining the Problem: Rock-Paper-Scissors\n",
    "\n",
    "What problem do we want to solve, exactly?  We want to use Neural Networks to recognize hand gestures, more specifically we want the network to automatically predict which of the three âœŠâœ‹âœŒï¸ gestures is shown. Hence, the output is a classification of the input image in one of the three classes. \n",
    "\n",
    "In the following, we will adopt the convention that \n",
    "- class 0 is âœŠ rock\n",
    "- class 1 is âœ‹ paper\n",
    "- class 2 is âœŒï¸ scissors\n",
    "\n",
    "and we will store example images (for training and testing) of each class in the corresponding folders `rock`, `paper`, and `scissors`.\n",
    "\n",
    "\n",
    "## 1. Collecting Rock-Paper-Scissor Examples\n",
    "\n",
    "In this section, we will collect examples of what rocks (âœŠ), papers (ðŸ¤š), and scissors (âœŒï¸) look like. To do this, we will need to enlist you're help as a Team Dataâ„¢ï¸ member. We will need to collect a good amount of images of each of these hand positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 How To Take The Pictures\n",
    "\n",
    "- We don't need high resolution images: use the lowest resolution/quality allowed by your phone (this reduces the size of the dataset and speeds up data transfer).\n",
    "- The hand should be more or less in the center of the image; it should net fill the whole image, but it should also not be too small either.\n",
    "![guide](utility/pics/sign_image_guide.png)\n",
    "- We want the dataset to represent as much variability as possible: if we want the classifier to work for all hand orientations, try to have examples for all of them; if we want to  handle many different lighting conditions, try to have some pictures for different lightings.\n",
    "- Avoid poses that are ambiguous, unless you want to make your job harder: e.g. don't include images of paper or scissors taken from the side in the dataset.\n",
    "- Avoid having two images in the dataset that are almost the same: change the camera and hand pose at least a little bit; this is important because we will randomly split training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Storing and Scaling the Images\n",
    "\n",
    "Produce 5 examples of rocks (âœŠ), papers (ðŸ¤š), and scissors (âœŒï¸) as you can, keeping in mind the guidelines set above. Remember that we want to have as much reasonable variability in the dataset as possible.\n",
    "\n",
    "Let's create a new folder at `utility/data/raw` where we can store our images. Your example images (dataset) of each class will need to be stored in the corresponding folders `rock`, `paper`, and `scissors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs, mkdir\n",
    "from os.path import exists\n",
    "\n",
    "base = 'utility/data'\n",
    "raw = f'{base}/raw'\n",
    "dirs = ['rock', 'paper', 'scissors']\n",
    "\n",
    "if not exists(raw):\n",
    "    makedirs(raw, exist_ok=True)\n",
    "\n",
    "for sign in dirs:\n",
    "    path = f'{raw}/{sign}'\n",
    "    \n",
    "    if not exists(path):\n",
    "        mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Store the images you took of rocks (âœŠ), papers (ðŸ¤š), and scissors (âœŒï¸) in the correct folders in `utility/data/raw`. Then, run the following cell to produced rescaled images, which will be stored in `utility/data/rescaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from utility.util import load_image, resize_image, save_image\n",
    "\n",
    "\n",
    "rescaled = f'{base}/rescaled'\n",
    "\n",
    "for sign in dirs:\n",
    "    path = f'{rescaled}/{sign}'\n",
    "    \n",
    "    if not exists(path):\n",
    "        mkdir(path)\n",
    "\n",
    "for path, _, files in os.walk(raw):\n",
    "    sign = os.path.basename(os.path.dirname(input_path))\n",
    "\n",
    "    for file in files:\n",
    "        input_path = f'{path}/{file}'\n",
    "        output_path = f'{rescaled}/{sign}/{file}'\n",
    "        \n",
    "        # note! warnings about lossy conversion are ok\n",
    "        image = load_image(input_path)\n",
    "        image = resize_image(image)\n",
    "\n",
    "        save_image(output_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Create a Train-Validation Split\n",
    "\n",
    "Now, make sure that the images in `utility/data/rescaled` are of reasonable size, meaning each image is less than `1MB`. \n",
    "\n",
    "**Try this!** Split your rescaled images into `training` and `test` sets. To do so, create two corresponding new subfolders under `utility/data`. \n",
    "\n",
    "> Hint: we will need more data in `training` (80%) than in `test` (20%), but each folder should have _distinct_ pictures of each team. Pictures in `test` cannot be the same as in `training` (this is your train-test/train-validation split of the data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates the folders for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_sets = ['training', 'test']\n",
    "\n",
    "for name in data_sets:\n",
    "    for sign in dirs:\n",
    "        path = f'{base}/{name}/{sign}'\n",
    "        if not exists(path):\n",
    "            os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Deploying the Images\n",
    "\n",
    "**Try this!** Now, upload your _rescaled images_ to the **shared box folder** ([here](https://wustl.box.com/s/a1vqlkp1qp6pfev6gn8uazbua9krjj75)). Again, ensure that each image is in their correct folder corresponding to `validation` or `training` and their respective classes `rock`, `paper`, and `scissors`. Team Modelâ„¢ï¸ will use these images to train your neural network.\n",
    "\n",
    "**Do this now, as we will be adding more images to the `rescaled` directory!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making Data from Data\n",
    "\n",
    "As we know from Lab 9, neural networks have a massive number of parameters that need to be tuned. As we also know, models with high degrees of flexibility, large numbers of parameters, require larger and larger datasets to prevent them from overfitting, an thus improve their performance. In our case, we know that we have a limited amount of data (as many as we could preduce in section 1). How can we find a middle ground between our lack of data and the ideal amount of data to train our models with.\n",
    "\n",
    "One way to do this is _data augmentation_, which - simply put - performs a bunch of different transformations on our original data in `training` to produce more images to train our neural network with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Augmentation\n",
    "#### Option 1: Using an External Tool\n",
    "\n",
    "We will use a new terminal window for this section of the lab. We will use a tool called [Image Augmentor](https://github.com/codebox/image_augmentor) to do this. Then, in the same terminal window, navigate to the `utility/image_augmentor` directory. Once you're there, read [the documentation](https://github.com/codebox/image_augmentor) and try a few different data augmentions on the pictures you added to `utility/data/training`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Use the Image Augmentor tool in reference to [it's documentation](https://codebox.net/pages/image-augmentation-with-python) to build an augmented dataset with each of the transformations provided.\n",
    "\n",
    "For each image in `utility/data/training`, create the following transformed versions:\n",
    "* horizonatally and vertically flipped\n",
    "* add noise (_not too much that the resulting images don't get too large!_)\n",
    "* rotations\n",
    "* translations\n",
    "* several zoomed versions\n",
    "* several blurred versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using `skimage`\n",
    "\n",
    "We can use the `skimage` package to transform your images as well. Here is the link to [an article](https://www.kaggle.com/tomahim/image-manipulation-augmentation-with-skimage) that describes some of the transformations that you can do.\n",
    "\n",
    "**Try this!** In the following cells, explore the transformations provided by `skimage`, create at **least 5 different augmented versions** of each of your images, and save your transformed images to `utility/data/rescaled`, appending a description of the transformation applied to the the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rescale\n",
    "from skimage.util import random_noise\n",
    "from utility.util import load_image, square_image, resize_image, show_image, save_image\n",
    "\n",
    "\n",
    "def show_images(before, after, op):\n",
    "    '''Displays before and after images. Use this to visualize what each transform is doing.'''\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    ax = axes.ravel()\n",
    "    ax[0].imshow(before, cmap='gray')\n",
    "    ax[0].set_title(\"Original image\")\n",
    "\n",
    "    ax[1].imshow(after, cmap='gray')\n",
    "    ax[1].set_title(op + \" image\")\n",
    "    if op == \"Rescaled\":\n",
    "        ax[0].set_xlim(0, 400)\n",
    "        ax[0].set_ylim(300, 0)\n",
    "    else:        \n",
    "        ax[0].axis('off')\n",
    "        ax[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# for example\n",
    "image = load_image('utility/pics/c0.png')\n",
    "show_image(image)\n",
    "\n",
    "squared = square_image(image)\n",
    "resized = resize_image(image)\n",
    "show_images(image, resized, 'Resized')\n",
    "\n",
    "noisy = random_noise(resized)\n",
    "show_images(resized, noisy, 'Noisy')\n",
    "\n",
    "# save the image (make sure they are saved in the folder of their sign)\n",
    "transformation_applied = 'Noisy'\n",
    "save_image(f'{rescaled}/rock_{transformation_applied}.png', noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Deploy Augmented Data\n",
    "\n",
    "Now, make sure that the images in `utility/data/training` include the original and at least 5 augmented versions each plus that all of them are of _reasonable size_, $\\approx$ `1MB`. If the size is bigger, rescale them again or replace them with other transfomations (e.g. add less noise).\n",
    "\n",
    "**Try this!** Now, upload your _augmented rescaled images_ along with the scaled version of the original to the **shared box folder** ([here](https://wustl.box.com/s/6sshfntf5lz8xxtv3oj2cl5nyr27bg8z)), ensuring that each image is in their correct folder. Team Modelâ„¢ï¸ will use these images to train your augmented neural network.\n",
    "\n",
    "> Hint: All the augmented data goes into `augmented`. No train-validation split required, since we can use our original (unaugmented) validateion set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
