{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10: Evaluating our Gesture Recognition NNs 🕸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:Xiaowen Ma\n",
    "\n",
    "Student ID:465589\n",
    "\n",
    "Collaborators:None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In our _last_ homework (woohoo!), we will be analyzing and evaluating the gesture recognition data and models created in `Lab10`. This is a great opportunity to recap the **Data Science workflow** with all its major aspects: \n",
    "\n",
    "- exploratory data analysis (EDA) and data profiling\n",
    "- machine learning workkflow\n",
    "- training, validation, testing data\n",
    "- model comparison\n",
    "- presenting results (creating plot)\n",
    "\n",
    "It will be extremely helpful to review **Lab 10 (Gesture Recognition with Neural Networks)** first.\n",
    "\n",
    "In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, a legend).\n",
    "\n",
    "Furthermore, in addition to recording your collaborators on this homework, please also remember to cite/indicate all external sources used when finishing this assignment. This includes peers, TAs, and links to online sources. Note that these citations will not free you from your obligation to submit your _own_ code and write-ups, however, they will be taken into account during the grading and regrading process.\n",
    "\n",
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Feel free to add as many cells as you need to** — just make sure you don't change what we gave you. \n",
    "* **Does it spark joy?** Note that you will be partially graded on the presentation (_cleanliness, clarity, comments_) of your notebook so make sure you [Marie Kondo](https://lifehacker.com/marie-kondo-is-not-a-verb-1833373654) your notebook before submitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The data needed for this assignemnt can be found [here](https://wustl.box.com/s/q8mnl1o2zq2bh0ca5zajtk3msnu03ou8). All of it was gathered in `Homework 10 (Part I)`: \n",
    "- training\n",
    "- validation\n",
    "- augmented\n",
    "- testing\n",
    "\n",
    "Here are the neural network models trained on `training`:\n",
    "- cse217_v1.h5 (still training; watch for announcement on Piazza)\n",
    "- cse217_v2.h5 (still training; watch for announcement on Piazza)\n",
    "\n",
    "Here are the neural network models trained on `augmented`:\n",
    "- cse217_v1_augmented.h5 (still training; watch for announcement on Piazza)\n",
    "- cse217_v2_augmented.h5 (still training; watch for announcement on Piazza)\n",
    "\n",
    "Note that to train these models we used the `validation` dataset to determine when to stop the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Data Collection, Data Profiling, and Model Understanding\n",
    "\n",
    "In this section, we will get a feel for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 0\n",
    "\n",
    "Following the instructions in `Lab10_DataAquisition` take 15 images of rock, paper, and scissors gestures (cf. `1.1 How To Take The Pictures`) and scale them using the provided code (`1.2 Storing, Scaling, and Sharing the Images`). Store them in a folder called `testing` along with the already collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs, mkdir\n",
    "from os.path import exists\n",
    "\n",
    "base = 'utility/data'\n",
    "raw = f'{base}/raw'\n",
    "dirs = ['rock', 'paper', 'scissors']\n",
    "\n",
    "if not exists(raw):\n",
    "    makedirs(raw, exist_ok=True)\n",
    "\n",
    "for sign in dirs:\n",
    "    path = f'{raw}/{sign}'\n",
    "    \n",
    "    if not exists(path):\n",
    "        mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Store the images you took of rocks (✊), papers (🤚), and scissors (✌️) in the correct folders in `utility/data/raw`. Then, run the following cell to produced rescaled images, which will be stored in `utility/data/testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/paper/IMG_4464.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/paper/IMG_4251.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/paper/IMG_4252.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/rock/IMG_4466.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/rock/IMG_4249.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/rock/IMG_4248.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/scissors/IMG_4465.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/scissors/IMG_4250.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/scissors/IMG_4457.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from utility.util import load_image, resize_image, save_image\n",
    "\n",
    "\n",
    "testing = f'{base}/testing'\n",
    "\n",
    "for sign in dirs:\n",
    "    path = f'{testing}/{sign}'\n",
    "    \n",
    "    if not exists(path):\n",
    "        makedirs(path, exist_ok=True)\n",
    "\n",
    "for path, _, files in os.walk(raw):\n",
    "    sign = os.path.basename(path)\n",
    "\n",
    "    for file in files:\n",
    "        if '.DS_Store' not in file:\n",
    "            input_path = f'{path}/{file}'\n",
    "            output_path = f'{testing}/{sign}/{file}'\n",
    "        \n",
    "            # note! warnings about lossy conversion are ok\n",
    "            image = load_image(input_path)\n",
    "            image = resize_image(image, (500, 500))\n",
    "\n",
    "            save_image(output_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "**Write-up!**  Report the number of images per class in each of the four datasets. Are the dataset balanced? No code submission required.\n",
    "> Hint: For most of this you can use the code from `Lab10_Model` with light modifications. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                  | training | validation | testing | augmented\n",
    "        rock (c0) |266          66           3         1237\n",
    "       paper (c1) |266          66           3         1255   \n",
    "    scissors (c2) |264          66           3         1258\n",
    "\n",
    "# your response here\n",
    "I think the dataset is balanced since within each dataset, the numbers of images for the three classes are about the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Now, let's look at our models. \n",
    "\n",
    "**Write-up!**  Compare the following statistics for all four models: \n",
    "- number of parameters\n",
    "- number of convolutional layers\n",
    "- number of dense layers\n",
    "- size of the model (`.h5`) file \n",
    "\n",
    "What are the most surprising aspects of these statistics to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,741,571\n",
      "Trainable params: 1,741,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import skimage\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Setup to show interactive jupyter widgets\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "def imgplotList(i,data):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(data[i],interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "    \n",
    "# Load a saved model\n",
    "cse217_v1=load_model('utility/cse217_v1.h5')\n",
    "cse217_v2=load_model('utility/cse217_v2.h5')\n",
    "cse217_v1_augmented=load_model('utility/cse217_v1_augmented.h5')\n",
    "cse217_v2_augmented=load_model('utility/cse217_v2_augmented.h5')\n",
    "#model = load_model(f\"{modelname}.h5\", compile=False)\n",
    "\n",
    "#cse217_v1.summary()\n",
    "#cse217_v2.summary()\n",
    "#cse217_v1_augmented.summary()\n",
    "cse217_v2_augmented.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      | # parameters | # conv layers | # dense layers | file size\n",
    "            cse217_v1 |24615           5                2               366,704 bytes (369 KB on disk)\n",
    "            cse217_v2 |1741571         3                2               20,954,384 bytes (21.4 MB on disk)\n",
    "  cse217_v1_augmented |24,615          5                2               366,896 bytes (401 KB on disk)\n",
    "  cse217_v2_augmented |1,741,571       3                2               20,954,384 bytes (21.9 MB on disk)   \n",
    "\n",
    "# your response here\n",
    "Models with V2 have way more parameters than those models of v1, and the sizes for v2 models are larger, while v2 models have fewer conv layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison: v1 vs v2\n",
    "\n",
    "By now we should know all of the ins and outs about our datasets and models (right?). Let's evaluate and compare the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "First let's investiage which of the two versions `cse217_v1` or `cse217_v2` performs better in the non-augmented setting. You can use the code provided in the *updated version* of  `Lab10_Model` under `5. Evaluate Neural Network on Validation Data` with light modifications. \n",
    "\n",
    "**Write-up** For both versions report the accuracy on all three datasets `training`, `validation`, and `testing` and summarize your findings. \n",
    "- Which model performs better? Justify your answer based on the presented accuraccies. \n",
    "- Argue whether we can be happy with the perfomrance of our model. If yes, justify why, if no, give suggestions on how to imporve the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we provide an example of how to load the testing. Note the dimensions of the dataset (especially the size of the images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/testing/paper/IMG_4464.jpg\n",
      "utility/data/testing/paper/IMG_4251.jpeg\n",
      "utility/data/testing/paper/IMG_4252.jpeg\n",
      "utility/data/testing/rock/IMG_4466.jpg\n",
      "utility/data/testing/rock/IMG_4249.jpeg\n",
      "utility/data/testing/rock/IMG_4248.jpeg\n",
      "utility/data/testing/scissors/IMG_4465.jpg\n",
      "utility/data/testing/scissors/IMG_4250.jpeg\n",
      "utility/data/testing/scissors/IMG_4457.jpg\n"
     ]
    }
   ],
   "source": [
    "from utility.util import load_dataset\n",
    "\n",
    "target_shape = (500, 500)\n",
    "X_test_example, y_test_example = load_dataset('utility/data/testing', target_shape)\n",
    "\n",
    "#X_train_example,y_train_example=load_dataset('utility/data/training',target_shape)\n",
    "\n",
    "#X_val_example,y_val_example=load_dataset('utility/data/validation',target_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures predicted correctly by model: 4\n",
      "Number of picutres in the dataset: 9\n",
      "0.4444444444444444\n",
      "Number of pictures predicted correctly by model: 3\n",
      "Number of picutres in the dataset: 9\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#print(X_test_example.shape)\n",
    "\n",
    "def accuracy(X,y, model):\n",
    "    acc = 0\n",
    "    label = y\n",
    "    image = X\n",
    "    patchsize = model.input_shape[1]\n",
    "    for i in range(0,len(X)):\n",
    "        image_tran = skimage.transform.resize(image[i], (patchsize,patchsize))\n",
    "        outs = model.predict(np.array([image_tran]))\n",
    "        predicted = np.argmax(outs)\n",
    "        if predicted == np.argmax(label[i]):\n",
    "            acc+=1\n",
    "    \n",
    "    print(\"Number of pictures predicted correctly by model: %d\" % acc)\n",
    "    print(\"Number of picutres in the dataset: %d\" % len(X))\n",
    "\n",
    "    return acc/len(X)\n",
    "\n",
    "\n",
    "# Compute Model Performance\n",
    "a = accuracy(X_test_example,y_test_example,cse217_v1) \n",
    "print(a)\n",
    "#b=accuracy(X_train_example,y_train_example,cse217_v1) \n",
    "#print(b)\n",
    "#c=accuracy(X_val_example,y_val_example,cse217_v1) \n",
    "#print(c)\n",
    "\n",
    "aa = accuracy(X_test_example,y_test_example,cse217_v2) \n",
    "print(aa)\n",
    "#bb = accuracy(X_train_example,y_train_example,cse217_v2) \n",
    "#print(bb)\n",
    "#cc=accuracy(X_val_example,y_val_example,cse217_v2) \n",
    "#print(cc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              |      training acc        |      validation acc     |       testing acc \n",
    "    cse217_v1 |    0.4642857142857143          0.4307692307692308           0.44444444\n",
    "    cse217_v2 |    0.5739795918367347          0.4717948717948718           0.33333333\n",
    "\n",
    "# your response here\n",
    "The v2 version performs better on training data and validation data, however, for testing data, v1 model performs slightly better. I don't think either one performs well enough since the accuracy are only about 0.5(actually neigther reaches 0.5), however, if we ramdomly guess the result, the accuracy would also be about 0.333333. We can improve our model's performance by incereasing the training data size, and we can also select from those 1741571 features(model-based feature selection; choose the most relevant ones, ignoring noisy features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Now, that we have summarized and analyzed the average performance of the models, let's look at individual images. \n",
    "\n",
    "**Write-up**  Using your own `testing` set and the better performing version that you identified in the previous problem, which of the three classes get predicted more correctly, which of the classes get mistaken for what other classes more frequently? \n",
    "\n",
    "> Hint: you may use the visualization implemented in the *updated version* of  `Lab10_Model` under `5. Evaluate Neural Network on Validation Data` (last code cell).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e28e1afe334cfea3be78c4e892e78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='i', max=8), Output()), _dom_cla…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.resultsShow(i, X, y, model)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show results by processing a single validataion or testing image\n",
    "names = [\"paper\", \"rock\", \"scissors\"]\n",
    "\n",
    "result = []        \n",
    "for i in range(len(y_test_example)):\n",
    "    if y_test_example[i,0] == 1:\n",
    "        result.append(0)\n",
    "    elif y_test_example[i,1] == 1:\n",
    "        result.append(1)\n",
    "    elif y_test_example[i,2] == 1:\n",
    "        result.append(2)\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, X,y, model):\n",
    "    guide = { 0:\"paper\",1:\"rock\",2:\"scissor\"}\n",
    "    #d = data.iloc[i]\n",
    "    #im = d[\"image\"]\n",
    "    #l = d[\"label\"]\n",
    "    im=X[i];\n",
    "    l=y[i];\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
    "    \n",
    "    imt = imr = skimage.transform.resize(im, (model.input_shape[1],model.input_shape[1]))\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    outs = model.predict(np.array([imt]))\n",
    "    predicted = np.argmax(outs)\n",
    "    print(outs)\n",
    "    print(\"predicted label, %s\" % guide.get(predicted))\n",
    "    print(\"actual label, %s\"% guide.get(l))\n",
    "\n",
    "    axs[2].bar(np.array(range(len(names)))-0.5, outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))    \n",
    "\n",
    "# Visualize Predictions    \n",
    "#print(\"Results on individual {} inputs: \".format(dataset_val.loc[0].dn))  \n",
    "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(X_test_example)-1, step=1, value=0, continuous_update=False), X=fixed(X_test_example), y=fixed(result), model=fixed(cse217_v2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "In my testing cases, i have three images for each class. Two the papers was predicted correctly; One rock was predicted correctly; None of the scissors were predicted correctly. One of the paper was mispredicted to be rock; Two of the rock was mis-predicted as paper and Two of the scissor was mis-predicted as paper as well, the other scissors was mis-predicted as rock. It seems that, if the image is not paper, it might be mis-predicted as paper more frequently. And scissors class was the worst class our model performs on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison: original vs augmented\n",
    "\n",
    "Now, let's investiage whether data augmentation imporves performance. \n",
    "\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "Which of the models `cse217_vx`  or `cse217_vx_augmented` for both versions performs better? You can again use the code provided in the *updated version* of `Lab10_Model` under `5. Evaluate Neural Network on Validation Data` with light modifications. \n",
    "\n",
    "**Write-up** Report and compare the accuracy on all three datasets `training`, `validation`, and `testing` of the original and the augmented model for both versions. Summarize your findings. \n",
    "- Did data augemntation help? \n",
    "- Which of the two NN versions benefited or suffered more from data augmentation? \n",
    "- Give an explanation/guestimate why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing v1\n",
      "Number of pictures predicted correctly by model: 6\n",
      "Number of picutres in the dataset: 9\n",
      "0.6666666666666666\n",
      "v2\n",
      "Number of pictures predicted correctly by model: 4\n",
      "Number of picutres in the dataset: 9\n",
      "0.4444444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"train v1\")\\nb1=accuracy(X_train_example,y_train_example,cse217_v1_augmented) \\nprint(b1)\\nprint(\"v2\")\\nb2=accuracy(X_train_example,y_train_example,cse217_v2_augmented) \\nprint(b2)\\nprint(\"validation v1\")\\nc1=accuracy(X_val_example,y_val_example,cse217_v1_augmented) \\nprint(c1)\\nprint(\"v2\")\\nc2=accuracy(X_val_example,y_val_example,cse217_v2_augmented) \\nprint(c2)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_val_example,y_val_example=load_dataset('utility/data/validation',target_shape)\n",
    "print(\"testing v1\")\n",
    "a1 = accuracy(X_test_example,y_test_example,cse217_v1_augmented) \n",
    "print(a1)\n",
    "print(\"v2\")\n",
    "a2 = accuracy(X_test_example,y_test_example,cse217_v2_augmented) \n",
    "print(a2)\n",
    "'''\n",
    "print(\"train v1\")\n",
    "b1=accuracy(X_train_example,y_train_example,cse217_v1_augmented) \n",
    "print(b1)\n",
    "print(\"v2\")\n",
    "b2=accuracy(X_train_example,y_train_example,cse217_v2_augmented) \n",
    "print(b2)\n",
    "print(\"validation v1\")\n",
    "c1=accuracy(X_val_example,y_val_example,cse217_v1_augmented) \n",
    "print(c1)\n",
    "print(\"v2\")\n",
    "c2=accuracy(X_val_example,y_val_example,cse217_v2_augmented) \n",
    "print(c2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      |     training acc     |     validation acc     |     testing acc \n",
    "            cse217_v1 | 0.4642857142857143      0.4307692307692308        0.4444444444444444\n",
    "  cse217_v1_augmented | 0.6556122448979592      0.6307692307692307        0.6666666666666666\n",
    "  \n",
    "  \n",
    "                      |     training acc     |     validation acc     |     testing acc \n",
    "            cse217_v2 | 0.5739795918367347      0.4717948717948718        0.3333333333333333 \n",
    "  cse217_v2_augmented | 0.798469387755102       0.7282051282051282        0.4444444444444444\n",
    "\n",
    "# your response here\n",
    "In general, augmented version helps. For both v1 and v2 models, the augmented version perform better on each dataset. And v2 is improved slightly more after augmentation. This may be because v2 uses more features than v1, thus, after augmentation, the performance of v2 will be increased more. However, the improvement of the accuracy of test data is not as large as the improvement on other dataset, and still, the test accuracy is poor(especially for v2, after augmentation, the accuracy of v2 is still onlt 0.444444), the reason might be we include too many parameters some of which might be noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "Now, let's have some fun! \n",
    "\n",
    "Let's explore a _real-time_ version of the model you identified as performing best running with your webcam. Open a new terminal window (on Mac OS you will need to use the built-in terminal app) and navigate to the directory, where you stored the model. Once there, run the following command, substituting `<model_name>` for the name of the file containing your model:\n",
    "\n",
    "```\n",
    "$ python(3) realtime.py <model_name>\n",
    "```\n",
    "\n",
    "Have fun!\n",
    "\n",
    "Note, `realtime.py` uses opencv, so you miht need to install it: \n",
    "\n",
    "- **opencv**: `pip(3) install opencv-python`\n",
    "\n",
    "\n",
    "**Write-up**  Summarize the performance of our NN model. \n",
    "- When does it work well, when does it have difficulties in predicting the correct gesture? Consider angle, background, and distance in your answer.  \n",
    "- Which of the three classes get predicted more correctly, which of the classes get mistaken for what other classes more frequently? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Remember to review your work and make sure it is well presented and organized. Not everyting you coded up needs to remain in your submission, infact for this hw, we arenot expecting any code submission. **[Does [this cell] spark joy?](https://i.kinja-img.com/gawker-media/image/upload/s--iW_3HGbT--/c_scale,dpr_2.0,f_auto,fl_progressive,q_80,w_800/oruf4oavtj5vpmvaquew.jpg)** You are always trying to communicate your findings to somebody, _maybe even yourself_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
