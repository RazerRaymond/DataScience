{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10: Evaluating our Gesture Recognition NNs 🕸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Ray Zhang\n",
    "\n",
    "Student ID: 458445\n",
    "\n",
    "Collaborators:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In our _last_ homework (woohoo!), we will be analyzing and evaluating the gesture recognition data and models created in `Lab10`. This is a great opportunity to recap the **Data Science workflow** with all its major aspects: \n",
    "\n",
    "- exploratory data analysis (EDA) and data profiling\n",
    "- machine learning workkflow\n",
    "- training, validation, testing data\n",
    "- model comparison\n",
    "- presenting results (creating plot)\n",
    "\n",
    "It will be extremely helpful to review **Lab 10 (Gesture Recognition with Neural Networks)** first.\n",
    "\n",
    "In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, a legend).\n",
    "\n",
    "Furthermore, in addition to recording your collaborators on this homework, please also remember to cite/indicate all external sources used when finishing this assignment. This includes peers, TAs, and links to online sources. Note that these citations will not free you from your obligation to submit your _own_ code and write-ups, however, they will be taken into account during the grading and regrading process.\n",
    "\n",
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Feel free to add as many cells as you need to** — just make sure you don't change what we gave you. \n",
    "* **Does it spark joy?** Note that you will be partially graded on the presentation (_cleanliness, clarity, comments_) of your notebook so make sure you [Marie Kondo](https://lifehacker.com/marie-kondo-is-not-a-verb-1833373654) your notebook before submitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The data needed for this assignemnt can be found [here](https://wustl.box.com/s/q8mnl1o2zq2bh0ca5zajtk3msnu03ou8). All of it was gathered in `Homework 10 (Part I)`: \n",
    "- training\n",
    "- validation\n",
    "- augmented\n",
    "- testing\n",
    "\n",
    "Here are the neural network models trained on `training`:\n",
    "- cse217_v1.h5 (still training; watch for announcement on Piazza)\n",
    "- cse217_v2.h5 (still training; watch for announcement on Piazza)\n",
    "\n",
    "Here are the neural network models trained on `augmented`:\n",
    "- cse217_v1_augmented.h5 (still training; watch for announcement on Piazza)\n",
    "- cse217_v2_augmented.h5 (still training; watch for announcement on Piazza)\n",
    "\n",
    "Note that to train these models we used the `validation` dataset to determine when to stop the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Data Collection, Data Profiling, and Model Understanding\n",
    "\n",
    "In this section, we will get a feel for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 0\n",
    "\n",
    "Following the instructions in `Lab10_DataAquisition` take 15 images of rock, paper, and scissors gestures (cf. `1.1 How To Take The Pictures`) and scale them using the provided code (`1.2 Storing, Scaling, and Sharing the Images`). Store them in a folder called `testing` along with the already collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs, mkdir\n",
    "from os.path import exists\n",
    "\n",
    "base = 'utility/data'\n",
    "raw = f'{base}/raw'\n",
    "dirs = ['rock', 'paper', 'scissors']\n",
    "\n",
    "if not exists(raw):\n",
    "    makedirs(raw, exist_ok=True)\n",
    "\n",
    "for sign in dirs:\n",
    "    path = f'{raw}/{sign}'\n",
    "    \n",
    "    if not exists(path):\n",
    "        mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Store the images you took of rocks (✊), papers (🤚), and scissors (✌️) in the correct folders in `utility/data/raw`. Then, run the following cell to produced rescaled images, which will be stored in `utility/data/testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/scissors/4.jpg\n",
      "utility/data/raw/scissors/2.jpg\n",
      "utility/data/raw/scissors/5 (1).jpg\n",
      "utility/data/raw/scissors/3.jpg\n",
      "utility/data/raw/scissors/1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/rock/4.jpg\n",
      "utility/data/raw/rock/2.jpg\n",
      "utility/data/raw/rock/5.jpg\n",
      "utility/data/raw/rock/3.jpg\n",
      "utility/data/raw/rock/1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/paper/4.jpg\n",
      "utility/data/raw/paper/5 (2).jpg\n",
      "utility/data/raw/paper/2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/raw/paper/3.jpg\n",
      "utility/data/raw/paper/1.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from utility.util import load_image, resize_image, save_image\n",
    "\n",
    "\n",
    "testing = f'{base}/testing'\n",
    "\n",
    "for sign in dirs:\n",
    "    path = f'{testing}/{sign}'\n",
    "    \n",
    "    if not exists(path):\n",
    "        makedirs(path, exist_ok=True)\n",
    "\n",
    "for path, _, files in os.walk(raw):\n",
    "    sign = os.path.basename(path)\n",
    "\n",
    "    for file in files:\n",
    "        input_path = f'{path}/{file}'\n",
    "        output_path = f'{testing}/{sign}/{file}'\n",
    "        \n",
    "        # note! warnings about lossy conversion are ok\n",
    "        image = load_image(input_path)\n",
    "        image = resize_image(image, (500, 500))\n",
    "\n",
    "        save_image(output_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "**Write-up!**  Report the number of images per class in each of the four datasets. Are the dataset balanced? No code submission required.\n",
    "> Hint: For most of this you can use the code from `Lab10_Model` with light modifications. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "| training | validation | testing | augmented\n",
    "        rock (c0) |\n",
    "       paper (c1) |        \n",
    "    scissors (c2) |   \n",
    "\n",
    "\n",
    "# your response here\n",
    "testing dataset: 5 rock (c0), 5 paper (c1), 5 scissors (c2)\n",
    "training dataset: 266 rock (c0), 266 paper (c1), 264 scissors (c2)\n",
    "validation dataset: 66 rock (c0), 66 paper (c1), 66 scissors (c2)\n",
    "augmented dataset: 1237 rock (c0), 1255 paper (c1), 1258 scissors (c2)\n",
    "\n",
    "The datasets are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 5)       140       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 124, 124, 5)       230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 62, 62, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 5)         230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 5)         230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 5)         230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               23168     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 24,615\n",
      "Trainable params: 24,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 126, 126, 5)       140       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 124, 124, 5)       230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 62, 62, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 60, 60, 5)         230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 30, 30, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 5)         230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 5)         230       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               23168     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 24,615\n",
      "Trainable params: 24,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,741,571\n",
      "Trainable params: 1,741,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,741,571\n",
      "Trainable params: 1,741,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "v1_raw = load_model('/home/ray/Downloads/hwhw10/utility/model.v1.raw.h5', compile=False)\n",
    "v1_aug = load_model('/home/ray/Downloads/hwhw10/utility/model.v1.augmented.h5', compile=False)\n",
    "v2_raw = load_model('/home/ray/Downloads/hwhw10/utility/model.v2.raw.h5', compile=False)\n",
    "v2_aug = load_model('/home/ray/Downloads/hwhw10/utility/model.v2.augmented.h5', compile=False)\n",
    "\n",
    "v1_raw.summary()\n",
    "v1_aug.summary()\n",
    "v2_raw.summary()\n",
    "v2_aug.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Now, let's look at our models. \n",
    "\n",
    "**Write-up!**  Compare the following statistics for all four models: \n",
    "- number of parameters\n",
    "- number of convolutional layers\n",
    "- number of dense layers\n",
    "- size of the model (`.h5`) file \n",
    "\n",
    "What are the most surprising aspects of these statistics to you? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "| # parameters | # conv layers | # dense layers | file size\n",
    "            cse217_v1 |\n",
    "            cse217_v2 |        \n",
    "  cse217_v1_augmented |   \n",
    "  cse217_v2_augmented |   \n",
    "\n",
    "\n",
    "# your response here\n",
    "v1_raw parameters:24,615 | conv layers:5 | dense layers:2 | file size:20MB\n",
    "v1_aug parameters:24,615 | conv layers:5 | dense layers:2 | file size:367KB\n",
    "v2_raw parameters:1,741,571 | conv layers:3 | dense layers:2 | file size:20MB\n",
    "v2_aug parameters:1,741,571 | conv layers:3 | dense layers:2 | file size:367KB\n",
    "\n",
    "I am surprised about the models's file size seems unrelated with the number of parameters and layers count, with v1_raw and v1_ aug has the same number of parameters and layers count but drastically different file size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison: v1 vs v2\n",
    "\n",
    "By now we should know all of the ins and outs about our datasets and models (right?). Let's evaluate and compare the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "First let's investiage which of the two versions `cse217_v1` or `cse217_v2` performs better in the non-augmented setting. You can use the code provided in the *updated version* of  `Lab10_Model` under `5. Evaluate Neural Network on Validation Data` with light modifications. \n",
    "\n",
    "**Write-up** For both versions report the accuracy on all three datasets `training`, `validation`, and `testing` and summarize your findings. \n",
    "- Which model performs better? Justify your answer based on the presented accuraccies. \n",
    "- Argue whether we can be happy with the perfomrance of our model. If yes, justify why, if no, give suggestions on how to imporve the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we provide an example of how to load the testing. Note the dimensions of the dataset (especially the size of the images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, model):\n",
    "    acc = 0\n",
    "    label =  y\n",
    "    image = X\n",
    "    patchsize = model.input_shape[1]\n",
    "    for i in range(0,len(X)):\n",
    "        image_tran = skimage.transform.resize(image[i], (patchsize,patchsize))\n",
    "        outs = model.predict(np.array([image_tran]))\n",
    "        predicted = np.argmax(outs)\n",
    "        if predicted == np.argmax(label[i]):\n",
    "            acc+=1\n",
    "    \n",
    "    print(\"Number of pictures predicted correctly by model: %d\" % acc)\n",
    "    print(\"Number of picutres in the dataset: %d\" % len(X))\n",
    "\n",
    "    return acc/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility/data/testing/scissors/4.jpg\n",
      "utility/data/testing/scissors/2.jpg\n",
      "utility/data/testing/scissors/5 (1).jpg\n",
      "utility/data/testing/scissors/3.jpg\n",
      "utility/data/testing/scissors/1.jpg\n",
      "utility/data/testing/rock/4.jpg\n",
      "utility/data/testing/rock/2.jpg\n",
      "utility/data/testing/rock/5.jpg\n",
      "utility/data/testing/rock/3.jpg\n",
      "utility/data/testing/rock/1.jpg\n",
      "utility/data/testing/paper/4.jpg\n",
      "utility/data/testing/paper/5 (2).jpg\n",
      "utility/data/testing/paper/2.jpg\n",
      "utility/data/testing/paper/3.jpg\n",
      "utility/data/testing/paper/1.jpg\n",
      "Number of pictures predicted correctly by model: 2\n",
      "Number of picutres in the dataset: 15\n",
      "Number of pictures predicted correctly by model: 5\n",
      "Number of picutres in the dataset: 15\n",
      "v1_raw on test: \n",
      "0.13333333333333333\n",
      "v2_raw on test: \n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from utility.util import load_dataset\n",
    "import skimage\n",
    "import numpy as np\n",
    "\n",
    "target_shape = (500, 500)\n",
    "X_test_example, y_test_example = load_dataset('utility/data/testing', target_shape)\n",
    "v1_t = accuracy(X_test_example, y_test_example, v1_raw) \n",
    "v2_t = accuracy(X_test_example, y_test_example, v2_raw) \n",
    "print(\"v1_raw on test: \")\n",
    "print(v1_t)\n",
    "print(\"v2_raw on test: \")\n",
    "print(v2_t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "target_shape = (500, 500)\n",
    "X_train, y_train = load_dataset('/home/ray/Downloads/hwhw10/utility/data/training', target_shape)\n",
    "v1_train = accuracy(X_train, y_train, v1_raw) \n",
    "v2_train = accuracy(X_train, y_train, v2_raw) \n",
    "print(\"v1_raw on train: \")\n",
    "print(v1_train)\n",
    "print(\"v2_raw on train: \")\n",
    "print(v2_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "target_shape = (500, 500)\n",
    "X_valid, y_valid = load_dataset('/home/ray/Downloads/hwhw10/utility/data/validation', target_shape)\n",
    "v1_v = accuracy(X_valid, y_valid, v1_raw) \n",
    "v2_v = accuracy(X_valid, y_valid, v2_raw) \n",
    "print(\"v1_raw on validation: \")\n",
    "print(v1_v)\n",
    "print(\"v2_raw on validation: \")\n",
    "print(v2_v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "| training acc | validation acc | testing acc \n",
    "    cse217_v1 |\n",
    "    cse217_v2 | \n",
    "\n",
    "# your response here\n",
    "              | training acc | validation acc | testing acc \n",
    "    cse217_v1 |     0.4643   |      0.4308    |    0.13\n",
    "    cse217_v2 |     0.5740   |      0.4718    |    0.33\n",
    "v1_raw on test: \n",
    "0.13333333333333333\n",
    "v2_raw on test: \n",
    "0.3333333333333333\n",
    "v1_raw on train: \n",
    "0.4642857142857143 \n",
    "v2_raw on train: \n",
    "0.5739795918367347\n",
    "v1_raw on validation: \n",
    "0.4307692307692308\n",
    "v2_raw on validation: \n",
    "0.4717948717948718\n",
    "\n",
    "Based on the presented data, the v2 model performs better than the v1 model on the all non-augmented data.\n",
    "It behaved in this way because v2 model involving in more parameters and trained more time, that it \"learned\" more from data.\n",
    "\n",
    "I dont think we are satisfied with the current accuracy. One of the ways we can improve is to trained the model on more datasets, so it generates better to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Now, that we have summarized and analyzed the average performance of the models, let's look at individual images. \n",
    "\n",
    "**Write-up**  Using your own `testing` set and the better performing version that you identified in the previous problem, which of the three classes get predicted more correctly, which of the classes get mistaken for what other classes more frequently? \n",
    "\n",
    "> Hint: you may use the visualization implemented in the *updated version* of  `Lab10_Model` under `5. Evaluate Neural Network on Validation Data` (last code cell).  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "Of all the images of rocks, 5 predictions of rock are all incorrect,recognizing them as paper.\n",
    "Of all the images of sissors, 2 predictions are incorrect,recognizing them as paper, and 3 of them are correct.\n",
    "Of all the images of papers, 2 predictions are incorrect,recognizing them as sissors and rock, and 3 of them are correct.\n",
    "\n",
    "Based on my observation, rock and paper are mixed most frequently, with sissors and papers followed behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_testdata(path2folder, foldername):\n",
    "    dataset_directory = pathlib.Path(path2folder)\n",
    "\n",
    "    # Now check the data\n",
    "    ddir=dataset_directory/foldername\n",
    "    cdirs={}\n",
    "    cdirs.update({ddir/\"rock\":0,\n",
    "                  ddir/\"paper\":1,\n",
    "                  ddir/\"scissors\":2})\n",
    "\n",
    "    names = [\"rock\", \"paper\", \"scissors\"]\n",
    "\n",
    "    for cdir,cdir_class in cdirs.items():\n",
    "        assert cdir.exists()==1, str(cdir)+' does not exist'\n",
    "        print(\"Found directory {} containing class {}\".format(cdir,names[cdir_class]))\n",
    "\n",
    "    imagesize = 500\n",
    "    dataset1=[]\n",
    "    for cdir,cn in reversed(list(cdirs.items())):\n",
    "\n",
    "        for f in tqdm(list(cdir.glob(\"*\"))):\n",
    "            try:\n",
    "                im=skimage.io.imread(f)\n",
    "                h,w=im.shape[0:2] # height, width\n",
    "                sz=min(h,w)\n",
    "                im=im[(h//2-sz//2):(h//2+sz//2),(w//2-sz//2):(w//2+sz//2),:] # defines the central square\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    im=skimage.img_as_ubyte(skimage.transform.resize(im,(imagesize,imagesize))) # resize it to 500x500, whatever the original resolution\n",
    "            except:\n",
    "                warnings.warn(\"ignoring \"+str(f))\n",
    "                continue\n",
    "\n",
    "            dataset1.append({\n",
    "                \"file\": f,\n",
    "                \"label\": cn,\n",
    "                \"image\": im\n",
    "            })\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    dataset1 = pd.DataFrame(dataset1)\n",
    "    dataset1[\"dn\"] = dataset1[\"file\"].apply(lambda x: x.parent.parts[-2])\n",
    "    return dataset1\n",
    "\n",
    "# Show results by processing a single validataion or testing image\n",
    "names = [\"rock\", \"paper\", \"scissors\"]\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, data, model):\n",
    "    guide = { 0:\"rock\",1:\"paper\",2:\"scissor\"}\n",
    "    d = data.iloc[i]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
    "    \n",
    "    imt = imr = skimage.transform.resize(im, (model.input_shape[1],model.input_shape[1]))\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    outs = model.predict(np.array([imt]))\n",
    "    predicted = np.argmax(outs)\n",
    "    print(outs)\n",
    "    print(\"predicted label, %s\" % guide.get(predicted))\n",
    "    print(\"actual label, %s\"% guide.get(l))\n",
    "\n",
    "    axs[2].bar(np.array(range(len(names)))-0.5, outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 34.68it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory /home/ray/Downloads/hwhw10/utility/data/raw/rock containing class rock\n",
      "Found directory /home/ray/Downloads/hwhw10/utility/data/raw/paper containing class paper\n",
      "Found directory /home/ray/Downloads/hwhw10/utility/data/raw/scissors containing class scissors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 18.66it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 34.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_eval = \"raw\"\n",
    "base_d = pathlib.Path(\"/home/ray/Downloads/hwhw10/utility/data\")\n",
    "dataset_test = create_user_testdata(base_d,data_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on individual raw inputs: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fde0fb1e0b4de78025b3df6060c7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='i', max=14), Output()), _dom_cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.resultsShow(i, data, model)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "# Show results by processing a single validataion or testing image\n",
    "import matplotlib.pyplot as plt\n",
    "names = [\"rock\", \"paper\", \"scissors\"]\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, data, model):\n",
    "    guide = { 0:\"rock\",1:\"paper\",2:\"scissor\"}\n",
    "    d = data.iloc[i]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
    "    \n",
    "    imt = imr = skimage.transform.resize(im, (model.input_shape[1],model.input_shape[1]))\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    outs = model.predict(np.array([imt]))\n",
    "    predicted = np.argmax(outs)\n",
    "    print(outs)\n",
    "    print(\"predicted label, %s\" % guide.get(predicted))\n",
    "    print(\"actual label, %s\"% guide.get(l))\n",
    "\n",
    "    axs[2].bar(np.array(range(len(names)))-0.5, outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))    \n",
    "print(\"Results on individual {} inputs: \".format(dataset_test.loc[0].dn)) \n",
    "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(dataset_test)-1, step=1, value=0, continuous_update=False), data=fixed(dataset_test.sample(len(dataset_test))), model=fixed(v2_raw))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison: original vs augmented\n",
    "\n",
    "Now, let's investiage whether data augmentation imporves performance. \n",
    "\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "Which of the models `cse217_vx`  or `cse217_vx_augmented` for both versions performs better? You can again use the code provided in the *updated version* of `Lab10_Model` under `5. Evaluate Neural Network on Validation Data` with light modifications. \n",
    "\n",
    "**Write-up** Report and compare the accuracy on all three datasets `training`, `validation`, and `testing` of the original and the augmented model for both versions. Summarize your findings. \n",
    "- Did data augemntation help? \n",
    "- Which of the two NN versions benefited or suffered more from data augmentation? \n",
    "- Give an explanation/guestimate why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures predicted correctly by model: 6\n",
      "Number of picutres in the dataset: 15\n",
      "Number of pictures predicted correctly by model: 8\n",
      "Number of picutres in the dataset: 15\n",
      "v1_aug on test: \n",
      "0.4\n",
      "v2_aug on test: \n",
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "v1_t_aug = accuracy(X_test_example, y_test_example, v1_aug) \n",
    "v2_t_aug = accuracy(X_test_example, y_test_example, v2_aug) \n",
    "print(\"v1_aug on test: \")\n",
    "print(v1_t_aug)\n",
    "print(\"v2_aug on test: \")\n",
    "print(v2_t_aug)\n",
    "#v1_train_aug = accuracy(X_train, y_train, v1_aug) \n",
    "#v2_train_aug = accuracy(X_train, y_train, v2_aug) \n",
    "#print(\"v1_aug on train: \")\n",
    "#print(v1_train_aug)\n",
    "#print(\"v2_aug on train: \")\n",
    "#print(v2_train_aug)\n",
    "#v1_v_aug = accuracy(X_valid, y_valid, v1_aug) \n",
    "#v2_v_aug = accuracy(X_valid, y_valid, v2_aug) \n",
    "#print(\"v1_aug on validation: \")\n",
    "#print(v1_v_aug)\n",
    "#print(\"v2_aug on validation: \")\n",
    "#print(v2_v_aug)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "| training acc | validation acc | testing acc \n",
    "            cse217_v1 |\n",
    "  cse217_v1_augmented | \n",
    "  \n",
    "  \n",
    "                      | training acc | validation acc | testing acc \n",
    "            cse217_v2 |\n",
    "  cse217_v2_augmented | \n",
    "\n",
    "# your response here\n",
    "\n",
    "                        | training acc | validation acc | testing acc \n",
    "              cse217_v1 |     0.4643   |      0.4308    |    0.13\n",
    "    cse217_v1_augmented |     0.6556   |      0.6308    |    0.4\n",
    "      \n",
    "              cse217_v2 |     0.5740   |      0.4718    |    0.33\n",
    "    cse217_v2_augmented |     0.7985   |      0.7282     |    0.53\n",
    "\n",
    "\n",
    "In my opinion, the augmentation helped to increased the accuracy of our model, since most of the data output increased.\n",
    "v2 improved more from augmentation.\n",
    "they have different improvementsmay be because v1 model, v2 model take in different parameters counts into considerations, and after augmentation, the model can takes into more parameter into consideration, as they have different way of processing the information of data, which may be caused them to be benefited differently.\n",
    "but still, the augment's improvements are unclear. For example, it only increased 0.2 on testing dataset for v2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "Now, let's have some fun! \n",
    "\n",
    "Let's explore a _real-time_ version of the model you identified as performing best running with your webcam. Open a new terminal window (on Mac OS you will need to use the built-in terminal app) and navigate to the directory, where you stored the model. Once there, run the following command, substituting `<model_name>` for the name of the file containing your model:\n",
    "\n",
    "```\n",
    "$ python(3) realtime.py <model_name>\n",
    "```\n",
    "\n",
    "Have fun!\n",
    "\n",
    "Note, `realtime.py` uses opencv, so you miht need to install it: \n",
    "\n",
    "- **opencv**: `pip(3) install opencv-python`\n",
    "\n",
    "\n",
    "**Write-up**  Summarize the performance of our NN model. \n",
    "- When does it work well, when does it have difficulties in predicting the correct gesture? Consider angle, background, and distance in your answer.  \n",
    "- Which of the three classes get predicted more correctly, which of the classes get mistaken for what other classes more frequently? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Remember to review your work and make sure it is well presented and organized. Not everyting you coded up needs to remain in your submission, infact for this hw, we arenot expecting any code submission. **[Does [this cell] spark joy?](https://i.kinja-img.com/gawker-media/image/upload/s--iW_3HGbT--/c_scale,dpr_2.0,f_auto,fl_progressive,q_80,w_800/oruf4oavtj5vpmvaquew.jpg)** You are always trying to communicate your findings to somebody, _maybe even yourself_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
